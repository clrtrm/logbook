# logbook

#### 2023-04-13
- **A/B testing:** a **variation** of a webpage or app is compared against a **control version**; statistical analysis is used to determine which one generates more engagement / performs better.
- "*Testing one change at a time helps them pinpoint which changes had an effect on visitor behavior, and which ones did not. Over time, they can combine the effect of multiple winning changes from experiments to demonstrate the measurable improvement of a new experience over the old one*". [A/B testing](https://www.optimizely.com/optimization-glossary/ab-testing/), Optimizely.
- **CRO:** conversion rate optimization

#### 2023-04-23
- read [Why review code?](https://sophiebits.com/2018/12/25/why-review-code.html) by Sophie Alpert
  - bus factor: the minimum number of team members that have to suddenly disappear from a project before the latter stalls due to a lack of knowledgeable or competent personnel

#### 2023-04-25
- unboxed the Logitech MX Vertical as part of my *Logitech ergonomic mice benchmark*
- redoing The Odin Project from scratch
- `Foundations > Introduction > How this course will work` ([link](https://www.theodinproject.com/lessons/foundations-how-this-course-will-work))
  - *expectations management*: on the use of English, the importance of seeking out resources on your own, forgetting concepts and having to come back to them
  - *mindset:* "DO NOT SKIP ANYTHING"
- `Foundations > Introduction > Introduction to web development` ([link](https://www.theodinproject.com/lessons/foundations-introduction-to-web-development))
  - the web was announced on August 6, 1991
  - the first website was that of the CERN (Conseil européen pour la recherche nucléaire)
  - ALIWEB (Archie-Like Indexing for the Web) is considered the first Web search engine, as its predecessors were either built with different purposes (the Wanderer, Gopher) or were only indexers (Archie, Veronica and Jughead)
    - robots.txt: a standardized file that contains instructions for bots i.e. which portions of the website they can or cannot access; can be used along sitemaps; useful for search engines web crawlers; respect relies on voluntary compliance.
  - 1993: Mosaic!
  - 1994: Netscape! Internet Explorer (which reached a peak of 95% usage share by 2003)! W3C (*"lead the World Wide Web to its full potential by developing common protocols that promote its evolution and ensure its interoperability"*)!
  - 1996: CSS (allowing HTML to be semantic rather than both semantic and presentational -> better accessibility)! Flash (originally FutureSplash)!
  - 1998: Web Standards project (WaSP)
  - The IETF (Internet Engineering Task Force) is a standards organization for the Internet. It is responsible for the technical standards that make up the Internet protocol suite (TCP/IP).
  - TCP / IP stands for Transmission Control Protocol / Internet Protocol; a suite of communication protocols used to interconnect network devices on the internet.
  - security through obscurity: the (often dangerous) reliance in security engineering on design or implementation secrecy as the main method of providing security to a system of component
  - some sites like Google host a `humans.txt`file; previously, Google had a `killer-robots.txt` for the Terminator
  - Three keys: `User-agent`, `Allow`and `Disallow`
  - The wildcard `*` stands for all robots
  - The slash `/` stands for the whole website content
  - Comments can be used
  - It is possible to list multiple robots with their own rules
